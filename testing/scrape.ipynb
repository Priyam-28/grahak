{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc3fe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in c:\\users\\priyam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.53.0)\n",
      "Requirement already satisfied: pyee<14,>=13 in c:\\users\\priyam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from playwright) (13.0.0)\n",
      "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in c:\\users\\priyam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from playwright) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\priyam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyee<14,>=13->playwright) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain-community (c:\\Users\\Priyam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\Priyam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-community (c:\\Users\\Priyam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\Priyam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-community (c:\\Users\\Priyam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\Priyam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\priyam\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (6.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~angchain-community (c:\\Users\\Priyam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\Priyam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-community (c:\\Users\\Priyam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\Priyam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~angchain-community (c:\\Users\\Priyam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ip (c:\\Users\\Priyam\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install playwright\n",
    "%pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2203d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Scraping website: https://example.com\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing scraper with https://example.com...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:Scraping error: No module named 'playwright'\n",
      "ERROR:__main__:Async execution error: Error scraping website: No module named 'playwright'\n",
      "ERROR:__main__:Error during scraping: Error scraping website: No module named 'playwright'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping failed: Error scraping website: No module named 'playwright'\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain_community.tools.playwright.utils import create_async_playwright_browser\n",
    "from bs4 import BeautifulSoup\n",
    "import asyncio\n",
    "import logging\n",
    "import nest_asyncio\n",
    "from typing import Optional\n",
    "\n",
    "# Enable nested event loops (for Jupyter notebooks and similar environments)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PlaywrightScraper:\n",
    "    def __init__(self):\n",
    "        self.async_browser = None\n",
    "        self.toolkit = None\n",
    "        self.tools_by_name = None\n",
    "    \n",
    "    async def _initialize_browser(self):\n",
    "        \"\"\"Initialize Playwright browser if not already initialized\"\"\"\n",
    "        if self.async_browser is None:\n",
    "            self.async_browser = await create_async_playwright_browser()\n",
    "            self.toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=self.async_browser)\n",
    "            tools = self.toolkit.get_tools()\n",
    "            self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "    \n",
    "    async def _close_browser(self):\n",
    "        \"\"\"Close the browser\"\"\"\n",
    "        if self.async_browser:\n",
    "            await self.async_browser.close()\n",
    "            self.async_browser = None\n",
    "            self.toolkit = None\n",
    "            self.tools_by_name = None\n",
    "\n",
    "def _run_async_function(coro):\n",
    "    \"\"\"Helper function to run async code in different contexts\"\"\"\n",
    "    try:\n",
    "        # Try to get the current event loop\n",
    "        loop = asyncio.get_event_loop()\n",
    "        if loop.is_running():\n",
    "            # If we're in a running loop (like Jupyter), create a task\n",
    "            import concurrent.futures\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                future = executor.submit(asyncio.run, coro)\n",
    "                return future.result()\n",
    "        else:\n",
    "            # If no loop is running, use asyncio.run\n",
    "            return asyncio.run(coro)\n",
    "    except RuntimeError:\n",
    "        # Fallback: create new event loop\n",
    "        return asyncio.run(coro)\n",
    "\n",
    "def scrape_website(website: str, timeout: int = 30000) -> Optional[str]:\n",
    "    \"\"\"Scrape website using Playwright with better error handling\"\"\"\n",
    "    logger.info(f\"Scraping website: {website}\")\n",
    "    \n",
    "    async def _scrape_async():\n",
    "        scraper = PlaywrightScraper()\n",
    "        try:\n",
    "            await scraper._initialize_browser()\n",
    "            \n",
    "            # Get the navigate tool\n",
    "            navigate_tool = scraper.tools_by_name.get(\"navigate_browser\")\n",
    "            if not navigate_tool:\n",
    "                raise Exception(\"Navigate tool not found in Playwright toolkit\")\n",
    "            \n",
    "            # Navigate to the website with timeout\n",
    "            logger.info(\"Navigating to website...\")\n",
    "            result = await navigate_tool.arun({\n",
    "                \"url\": website,\n",
    "                \"timeout\": timeout\n",
    "            })\n",
    "            logger.info(f\"Navigation result: {result}\")\n",
    "            \n",
    "            # Check if navigation was successful\n",
    "            if \"status code 200\" not in result and \"error\" in result.lower():\n",
    "                raise Exception(f\"Navigation failed: {result}\")\n",
    "            \n",
    "            # Try to get page content using get_elements tool\n",
    "            get_elements_tool = scraper.tools_by_name.get(\"get_elements\")\n",
    "            if get_elements_tool:\n",
    "                try:\n",
    "                    # Get the entire page HTML\n",
    "                    page_content = await get_elements_tool.arun({\n",
    "                        \"selector\": \"html\",\n",
    "                        \"attributes\": [\"outerHTML\"]\n",
    "                    })\n",
    "                    \n",
    "                    if page_content and isinstance(page_content, str):\n",
    "                        # Parse the JSON-like response\n",
    "                        import json\n",
    "                        try:\n",
    "                            parsed_content = json.loads(page_content.replace(\"'\", '\"'))\n",
    "                            if parsed_content and len(parsed_content) > 0:\n",
    "                                html_content = parsed_content[0].get(\"outerHTML\", \"\")\n",
    "                                if html_content:\n",
    "                                    logger.info(\"Website scraped successfully!\")\n",
    "                                    return html_content\n",
    "                        except json.JSONDecodeError:\n",
    "                            # If it's not JSON, maybe it's already HTML\n",
    "                            if \"<html\" in page_content.lower():\n",
    "                                return page_content\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"get_elements failed: {e}\")\n",
    "            \n",
    "            # Fallback: try extract_text tool for content\n",
    "            extract_text_tool = scraper.tools_by_name.get(\"extract_text\")\n",
    "            if extract_text_tool:\n",
    "                try:\n",
    "                    text_content = await extract_text_tool.arun({})\n",
    "                    if text_content:\n",
    "                        # Create minimal HTML structure with the text\n",
    "                        html_content = f\"<html><body>{text_content}</body></html>\"\n",
    "                        logger.info(\"Website text extracted successfully!\")\n",
    "                        return html_content\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"extract_text failed: {e}\")\n",
    "            \n",
    "            # Final fallback: try to access browser page directly\n",
    "            if hasattr(scraper.async_browser, 'contexts') and scraper.async_browser.contexts:\n",
    "                context = scraper.async_browser.contexts[0]\n",
    "                if context.pages:\n",
    "                    page = context.pages[0]\n",
    "                    html_content = await page.content()\n",
    "                    logger.info(\"Website scraped successfully via direct page access!\")\n",
    "                    return html_content\n",
    "            \n",
    "            raise Exception(\"Could not retrieve page content using any method\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Scraping error: {e}\")\n",
    "            raise Exception(f\"Error scraping website: {e}\")\n",
    "        finally:\n",
    "            await scraper._close_browser()\n",
    "    \n",
    "    # Run the async function with proper event loop handling\n",
    "    try:\n",
    "        return _run_async_function(_scrape_async())\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Async execution error: {e}\")\n",
    "        raise\n",
    "\n",
    "def extract_body_content(html_content: str) -> str:\n",
    "    \"\"\"Extract body content from HTML with error handling\"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        body_content = soup.body\n",
    "        if body_content:\n",
    "            return str(body_content)\n",
    "        return html_content  # Return full content if no body tag\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting body content: {e}\")\n",
    "        return html_content\n",
    "\n",
    "def clean_body_content(body_content: str) -> str:\n",
    "    \"\"\"Clean body content by removing scripts and styles\"\"\"\n",
    "    try:\n",
    "        soup = BeautifulSoup(body_content, \"html.parser\")\n",
    "\n",
    "        # Remove unwanted elements\n",
    "        for element in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\", \"noscript\"]):\n",
    "            element.extract()\n",
    "\n",
    "        # Get text content\n",
    "        cleaned_content = soup.get_text(separator=\"\\n\")\n",
    "        cleaned_content = \"\\n\".join(\n",
    "            line.strip() for line in cleaned_content.splitlines() \n",
    "            if line.strip() and len(line.strip()) > 3\n",
    "        )\n",
    "\n",
    "        return cleaned_content\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error cleaning content: {e}\")\n",
    "        return body_content\n",
    "\n",
    "def split_dom_content(dom_content: str, max_length: int = 6000) -> list[str]:\n",
    "    \"\"\"Split DOM content into chunks\"\"\"\n",
    "    if not dom_content:\n",
    "        return [\"\"]\n",
    "    \n",
    "    return [\n",
    "        dom_content[i : i + max_length] \n",
    "        for i in range(0, len(dom_content), max_length)\n",
    "    ]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set up logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Test with a simple website first\n",
    "    test_url = \"https://example.com\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"Testing scraper with {test_url}...\")\n",
    "        html_content = scrape_website(test_url)\n",
    "        \n",
    "        if html_content:\n",
    "            # Extract body content\n",
    "            body_content = extract_body_content(html_content)\n",
    "            \n",
    "            # Clean the content\n",
    "            cleaned_content = clean_body_content(body_content)\n",
    "            \n",
    "            # Split into chunks\n",
    "            chunks = split_dom_content(cleaned_content)\n",
    "            \n",
    "            print(f\"Scraped and processed {len(chunks)} chunks\")\n",
    "            print(f\"First chunk preview: {chunks[0][:200]}...\")\n",
    "        else:\n",
    "            print(\"Failed to scrape website\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during scraping: {e}\")\n",
    "        print(f\"Scraping failed: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
