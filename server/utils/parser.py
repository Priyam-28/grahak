from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
import logging

logger = logging.getLogger(__name__)

template = (
    "You are a friendly and helpful AI shopping assistant. "
    "A user is looking for products based on this query: '{user_query}'.\n\n"
    "Here is some product information I found: \n{product_context}\n\n"
    "Please analyze this information and provide a concise (2-4 sentences) response that directly addresses the user's query. "
    "Highlight how the found products relate to their request. "
    "Do not list the product details again; they will be shown separately. "
    "If the provided product information doesn't seem to fully match the user's query, acknowledge this and briefly explain what you found instead. "
    "If the product context is empty or indicates no products were found, inform the user clearly that no relevant products could be found for their query."
)

model = OllamaLLM(model="llama3.2") 

def get_llm_summary(product_context: str, user_query: str) -> str:
    """
    Generates a summary from the LLM based on product context and user query. (Non-streaming)
    """
    try:
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | model

        logger.info(f"Generating LLM summary for query: '{user_query}' with context of length: {len(product_context)}")
        response = chain.invoke({
            "product_context": product_context,
            "user_query": user_query
        })

        if isinstance(response, str):
            cleaned_response = response.strip()
            logger.info("LLM summary generated successfully.")
            return cleaned_response if cleaned_response else "No specific summary generated by AI."
        else:
            logger.warning(f"Unexpected LLM response type: {type(response)}")
            return "AI response format was unexpected."

    except Exception as e:
        logger.error(f"Error in get_llm_summary: {e}", exc_info=True)
        return f"Error during AI summary generation: {str(e)}"

async def stream_llm_summary(product_context: str, user_query: str):
    """
    Streams the LLM summary token by token. (Async generator)
    """
    try:
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | model # type: ignore

        logger.info(f"Streaming LLM summary for query: '{user_query}' with context of length: {len(product_context)}")

        async for token in chain.astream({
            "product_context": product_context,
            "user_query": user_query
        }):
            if isinstance(token, str):
                yield token


    except Exception as e:
        logger.error(f"Error in stream_llm_summary: {e}", exc_info=True)
        yield f"Error during AI summary streaming: {str(e)}"

def parse_with_ollama(dom_chunks: list[str], parse_description: str) -> str:
    logger.warning("Legacy parse_with_ollama called. Consider using get_llm_summary or stream_llm_summary.")
    if not dom_chunks:
        return "No content provided to parse."
    product_context = dom_chunks[0]
    user_query = parse_description

    return get_llm_summary(product_context, user_query)