from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
import logging

logger = logging.getLogger(__name__)

template = (
    "You are a friendly and helpful AI shopping assistant. "
    "A user is looking for products based on this query: '{user_query}'.\n\n"
    "Here is some product information I found: \n{product_context}\n\n"
    "Please analyze this information and provide a concise (2-4 sentences) response that directly addresses the user's query. "
    "Highlight how the found products relate to their request. "
    "Do not list the product details again; they will be shown separately. "
    "If the provided product information doesn't seem to fully match the user's query, acknowledge this and briefly explain what you found instead. "
    "If the product context is empty or indicates no products were found, inform the user clearly that no relevant products could be found for their query."
)

model = OllamaLLM(model="llama3.2") 

def get_llm_summary(product_context: str, user_query: str) -> str:
    """
    Generates a summary from the LLM based on product context and user query. (Non-streaming)
    """
    try:
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | model

        logger.info(f"Generating LLM summary for query: '{user_query}' with context of length: {len(product_context)}")
        response = chain.invoke({
            "product_context": product_context,
            "user_query": user_query
        })

        if isinstance(response, str):
            cleaned_response = response.strip()
            logger.info("LLM summary generated successfully.")
            return cleaned_response if cleaned_response else "No specific summary generated by AI."
        else:
            logger.warning(f"Unexpected LLM response type: {type(response)}")
            return "AI response format was unexpected."

    except Exception as e:
        logger.error(f"Error in get_llm_summary: {e}", exc_info=True)
        return f"Error during AI summary generation: {str(e)}"

async def stream_llm_summary(product_context: str, user_query: str):
    """
    Streams the LLM summary token by token. (Async generator)
    """
    try:
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | model # type: ignore

        logger.info(f"Streaming LLM summary for query: '{user_query}' with context of length: {len(product_context)}")

        async for token in chain.astream({
            "product_context": product_context,
            "user_query": user_query
        }):
            if isinstance(token, str):
                yield token


    except Exception as e:
        logger.error(f"Error in stream_llm_summary: {e}", exc_info=True)
        yield f"Error during AI summary streaming: {str(e)}"

def parse_with_ollama(dom_chunks: list[str], parse_description: str) -> str:
    logger.warning("Legacy parse_with_ollama called. Consider using get_llm_summary or stream_llm_summary.")
    if not dom_chunks:
        return "No content provided to parse."
    product_context = "\n".join(dom_chunks) # Join if multiple chunks
    user_query = parse_description

    return get_llm_summary(product_context, user_query)

# It's good practice to define the ProductData class where it's used or import it.
# Assuming ProductData is defined in scraper.py and we might face circular import issues if imported directly.
# For now, this parser will return a dictionary, and the scraper can construct the ProductData object.
# Or, we define a minimal ProductData here or pass fields.
# Let's try to import it carefully or expect a dict.
try:
    from .scraper import ProductData
except ImportError:
    logger.warning("Could not import ProductData from .scraper in parser.py. LLM parsing will return dict.")
    # Define a fallback or expect dict if ProductData cannot be imported
    ProductData = dict


extraction_template_json_str = """
You are an expert data extraction AI. Your task is to extract detailed product information from the provided text content, which is typically in Markdown format from a scraped webpage.
The user is searching for: '{user_query}'
The content is from the site: '{site_name_hint}' (e.g., Amazon, Flipkart, Myntra, or Generic e-commerce)

Extract the following fields and return them in a VALID JSON object. Ensure all string values are properly escaped.
If a field is not found or not applicable, use null or an empty string/list as appropriate for the field type.

Fields to extract:
- "title": (string) The full name or title of the product.
- "price": (string) The current selling price of the product (e.g., "4,999", "129.99"). Extract the numerical value if possible.
- "currency": (string) The currency of the price (e.g., "INR", "USD", "â‚¹", "$"). Default to "INR" if not specified.
- "rating": (string) The average customer rating, if available (e.g., "4.5 out of 5 stars", "4.2 stars").
- "reviews_count": (string) The total number of reviews or ratings, if available. (e.g., "1205 ratings"). Extract the numerical part.
- "availability": (string) The stock status or availability (e.g., "In Stock", "Out of Stock", "Only 2 left").
- "image_url": (string) The primary image URL of the product. Look for fully qualified URLs.
- "description": (string) A detailed description of the product, including key features and specifications. Aim for a comprehensive summary.
- "reviews": (list of strings) A list of individual customer review texts. Extract 3-5 representative reviews if available. Each review should be a string in the list.
- "seller": (string) The name of the seller, if specified (e.g., "Sold by XStore", "X Retail").

Here is the content to parse:
--- CONTENT START ---
{scraped_content_markdown}
--- CONTENT END ---

Respond ONLY with the valid JSON object. Do not include any explanatory text before or after the JSON.
Example of a field not found: "seller": null or "seller": ""
Example for reviews: "reviews": ["This is great!", "I loved the quality."]
"""

def parse_firecrawl_data_with_ollama(firecrawl_data: dict, user_query: str, site_name_hint: str) -> Optional[ProductData]:
    """
    Parses Firecrawl data (especially markdown) using Ollama to extract structured product information.
    Returns a ProductData object or None if parsing fails.
    """
    if not firecrawl_data:
        logger.warning("No Firecrawl data provided to parse.")
        return None

    # Prefer markdown content if available, otherwise use 'content'
    content_to_parse = firecrawl_data.get('markdown')
    if not content_to_parse:
        content_to_parse = firecrawl_data.get('content') # Raw HTML content
        if content_to_parse:
            # Basic cleaning if it's HTML (though Firecrawl usually provides markdown)
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(content_to_parse, 'html.parser')
            content_to_parse = soup.get_text(separator=' ', strip=True)
            logger.info("Used raw content from Firecrawl and stripped HTML for parsing.")
        else: # Check for llm_extraction if Firecrawl did its own
            llm_extraction = firecrawl_data.get('llm_extraction')
            if llm_extraction and isinstance(llm_extraction, dict):
                 logger.info("Using Firecrawl's own llm_extraction data.")
                 # Directly map if fields are compatible
                 # This part needs to be adapted based on actual llm_extraction structure from Firecrawl
                 return ProductData(
                     title=str(llm_extraction.get('name', llm_extraction.get('title', ''))),
                     price=str(llm_extraction.get('price', '')),
                     currency=str(llm_extraction.get('currency', 'INR')),
                     description=str(llm_extraction.get('description', '')),
                     image_url=str(llm_extraction.get('image_url', '')),
                     platform=site_name_hint,
                     product_url=firecrawl_data.get('product_url', ''),
                     raw_data=firecrawl_data,
                     llm_parsed_data=llm_extraction # Store this separately
                 )


    if not content_to_parse:
        logger.warning(f"No markdown or content found in Firecrawl data for URL: {firecrawl_data.get('product_url')}")
        return None

    # Ensure model is initialized (it's global in this file)
    if not model:
        logger.error("Ollama model not initialized in parser.")
        return None

    prompt = ChatPromptTemplate.from_template(extraction_template_json_str)
    chain = prompt | model # OllamaLLM is expected to output a string

    logger.info(f"Attempting to parse content for {firecrawl_data.get('product_url', 'Unknown URL')} using Ollama. Content length: {len(content_to_parse)}")

    # Truncate content_to_parse if it's excessively long to avoid issues with LLM context limits
    # Adjust MAX_CONTENT_LENGTH as needed, considering typical context window sizes (e.g., Llama3 8k tokens)
    # This is a rough character limit, actual token count will vary.
    MAX_CONTENT_LENGTH = 15000  # Approx 4k-5k tokens, should be safe for many models
    if len(content_to_parse) > MAX_CONTENT_LENGTH:
        logger.warning(f"Content length {len(content_to_parse)} exceeds max {MAX_CONTENT_LENGTH}. Truncating.")
        content_to_parse = content_to_parse[:MAX_CONTENT_LENGTH]

    try:
        response_str = chain.invoke({
            "user_query": user_query,
            "site_name_hint": site_name_hint,
            "scraped_content_markdown": content_to_parse
        })

        if not isinstance(response_str, str) or not response_str.strip():
            logger.error(f"Ollama returned an empty or non-string response: {response_str}")
            return None

        # The response should be a JSON string. Try to parse it.
        # Sometimes LLMs add ```json ... ``` around the response.
        cleaned_response_str = response_str.strip()
        if cleaned_response_str.startswith("```json"):
            cleaned_response_str = cleaned_response_str[7:]
        if cleaned_response_str.endswith("```"):
            cleaned_response_str = cleaned_response_str[:-3]

        cleaned_response_str = cleaned_response_str.strip()

        import json
        try:
            extracted_json = json.loads(cleaned_response_str)
        except json.JSONDecodeError as je:
            logger.error(f"Failed to decode JSON from Ollama response: {je}. Response was: \n---\n{cleaned_response_str}\n---")
            # Fallback: try to extract title at least if metadata is present
            title_from_meta = firecrawl_data.get('metadata', {}).get('title')
            if title_from_meta:
                 logger.warning("JSON parsing failed, but creating ProductData with title from metadata and raw content.")
                 return ProductData(
                    title=str(title_from_meta),
                    description="Could not parse full details. Content available in raw_data.",
                    platform=site_name_hint,
                    product_url=firecrawl_data.get('product_url', ''),
                    raw_data=firecrawl_data,
                    llm_parsed_data={'error': 'JSON parsing failed', 'raw_llm_output': cleaned_response_str}
                )
            return None


        # Create ProductData object from the extracted JSON
        # Handle potential missing keys gracefully
        product = ProductData(
            title=str(extracted_json.get('title', firecrawl_data.get('metadata', {}).get('title', ''))), # Fallback to metadata title
            price=str(extracted_json.get('price', '')),
            currency=str(extracted_json.get('currency', 'INR')),
            rating=str(extracted_json.get('rating', '')),
            reviews_count=str(extracted_json.get('reviews_count', '')),
            availability=str(extracted_json.get('availability', '')),
            image_url=str(extracted_json.get('image_url', '')),
            description=str(extracted_json.get('description', '')),
            reviews=extracted_json.get('reviews', []) if isinstance(extracted_json.get('reviews'), list) else [],
            seller=str(extracted_json.get('seller', '')),
            platform=site_name_hint, # Already passed
            product_url=firecrawl_data.get('product_url', ''), # Ensure this is set
            raw_data=firecrawl_data, # Store the original Firecrawl data
            llm_parsed_data=extracted_json # Store the LLM's structured output
        )
        logger.info(f"Successfully parsed data for {product.product_url} -> Title: {product.title}")
        return product

    except Exception as e:
        logger.error(f"Error during Ollama parsing for {firecrawl_data.get('product_url', 'Unknown URL')}: {e}", exc_info=True)
        # Fallback: try to extract title at least if metadata is present
        title_from_meta = firecrawl_data.get('metadata', {}).get('title')
        if title_from_meta:
            logger.warning("LLM invocation failed, but creating ProductData with title from metadata and raw content.")
            return ProductData(
                title=str(title_from_meta),
                description="Could not parse full details due to LLM error. Content available in raw_data.",
                platform=site_name_hint,
                product_url=firecrawl_data.get('product_url', ''),
                raw_data=firecrawl_data,
                llm_parsed_data={'error': f'LLM invocation/processing error: {str(e)}'}
            )
        return None